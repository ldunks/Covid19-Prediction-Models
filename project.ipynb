{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "\n",
    "path = \"../p2/\"\n",
    "files = {\n",
    "    'test':{\n",
    "        'in':\"cases_2021_test_processed_unlabelled_2\",\n",
    "        'out':\"Testing_Data\"\n",
    "    },\n",
    "    'train':{\n",
    "        'in':\"cases_2021_train_processed_2\",\n",
    "        'out':\"Training_Data\"\n",
    "    }\n",
    "}\n",
    "\n",
    "mode = 'train'\n",
    "\n",
    "filename = files[mode]['in']\n",
    "filetype = \".xlsx\"\n",
    "data = pd.read_excel(path + filename + filetype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Feature Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionWork = data.copy(deep=False)\n",
    "data = data.loc[:, ~data.columns.isin(['Confirmed', 'Deaths', 'Recovered', 'Active'])]\n",
    "\n",
    "def genderMapping(val):\n",
    "    return 1 if val == 'male' else 0\n",
    "\n",
    "def diseaseMapping(val):\n",
    "    return 1 if val else 0\n",
    "\n",
    "def outcomeMapping(val):\n",
    "    match val:\n",
    "        case 'deceased':\n",
    "            return 0\n",
    "        case 'hospitalized':\n",
    "            return 1\n",
    "        case _:\n",
    "            return 2\n",
    "        \n",
    "data['sex'] = np.vectorize(genderMapping)(data['sex'])\n",
    "data['chronic_disease_binary'] = np.vectorize(diseaseMapping)(data['chronic_disease_binary'])\n",
    "if 'outcome_group' in data:\n",
    "    data['outcome_group'] = np.vectorize(outcomeMapping)(data['outcome_group'])\n",
    "\n",
    "data['date_confirmation'] = data['date_confirmation'] - data['date_confirmation'].min()\n",
    "data['date_confirmation'] = data['date_confirmation'].dt.days.astype(float) / 149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionWork = regionWork.replace(np.nan, '')\n",
    "# regionWork['region'] = regionWork['province'] + ' ' + regionWork['country']\n",
    "\n",
    "regionWork = regionWork.loc[:, regionWork.columns.isin(['country', 'province', 'Incident_Rate', 'Case_Fatality_Ratio'])]\n",
    "\n",
    "regionWork = regionWork.groupby(['province', 'country'], as_index=False).mean()\n",
    "\n",
    "regionWork = regionWork.loc[:, regionWork.columns.isin(['country', 'Incident_Rate', 'Case_Fatality_Ratio'])]\n",
    "\n",
    "regionWork = regionWork.groupby(['country']).mean()\n",
    "\n",
    "\n",
    "\n",
    "rates = regionWork.loc[:, regionWork.columns.isin(['Incident_Rate'])].to_dict('index')\n",
    "\n",
    "ratios = regionWork.loc[:, regionWork.columns.isin(['Case_Fatality_Ratio'])].to_dict('index')\n",
    "\n",
    "def getRates(c):\n",
    "    return rates[c]['Incident_Rate']\n",
    "\n",
    "def getRatios(c):\n",
    "    return ratios[c]['Case_Fatality_Ratio']\n",
    "\n",
    "\n",
    "data['average_rate'] = data['country'].apply(getRates)\n",
    "data['average_ratio'] = data['country'].apply(getRatios)\n",
    "data = data.loc[:, ~data.columns.isin(['country', 'province'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(files[mode]['out'] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Class Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Read the mapped data file\n",
    "data = pd.read_csv(\"Training_Data.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "features = data.drop(columns=['outcome_group'])\n",
    "labels = data['outcome_group']\n",
    "\n",
    "# Use SMOTE to oversample\n",
    "sm = SMOTE(random_state=42);\n",
    "features_resampled, labels_resampled = sm.fit_resample(features, labels)\n",
    "\n",
    "# Combine features and labels into a new DataFrame\n",
    "data_resampled = pd.concat([pd.DataFrame(features_resampled, columns=features.columns), pd.DataFrame(labels_resampled, columns=['outcome_group'])], axis=1)\n",
    "\n",
    "# Save the oversampled dataset to a new CSV file\n",
    "data_resampled.to_csv(\"oversampled_data.csv\", index=False)\n",
    "\n",
    "print(\"Oversampled dataset saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid under and over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to SMOTEENN to create hybrid of over and under sampling\n",
    "sme = SMOTEENN(random_state=42);\n",
    "features_resampled, labels_resampled = sme.fit_resample(features, labels)\n",
    "\n",
    "# Combine features and labels into a new DataFrame\n",
    "data_resampled = pd.concat([pd.DataFrame(features_resampled, columns=features.columns), pd.DataFrame(labels_resampled, columns=['outcome_group'])], axis=1)\n",
    "\n",
    "# Save the oversampled dataset to a new CSV file\n",
    "data_resampled.to_csv(\"Hybrid_data.csv\", index=False)\n",
    "\n",
    "print(\"Hybrid dataset saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 - Building models and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "files['under'] = './Undersampled_data/Undersampled_data.csv'\n",
    "files['hybrid'] = './Hybrid_data/Hybrid_data.csv'\n",
    "files['over'] = './Oversampled_data/Oversampled_Data.csv'\n",
    "files['normal'] = './Mapped_data/Training_Data.csv'\n",
    "# files['verify'] = './p2/cases_2021_train_processed_2.xlsx'\n",
    "files['unlabelled'] = './Mapped_data/Testing_Data.csv'\n",
    "\n",
    "target = 'hybrid'\n",
    "test = 'normal'\n",
    "unlabelled = 'unlabelled'\n",
    "\n",
    "data = pd.read_csv(files[target])\n",
    "test = pd.read_csv(files[test])\n",
    "unlabelled_data = pd.read_csv(files[unlabelled])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('outcome_group', axis=1)\n",
    "y = data['outcome_group']\n",
    "xgb_X_test = test.drop('outcome_group', axis=1)\n",
    "xgb_y_test = test['outcome_group']\n",
    "\n",
    "xgb_X_train, xgb_X_verify, xgb_Y_train, xgb_Y_verify = train_test_split(X, y, test_size=0.2, random_state=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter tuning using RandomSearhCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid randomized\n",
    "xgb = XGBClassifier(objective='multi:softmax', nthread = 1)\n",
    "#xgb.fit(X_train, y_train)\n",
    "params = {\n",
    "        'learning_rate': [0.01, 0.02, 0.1, 0.2],\n",
    "        'min_child_weight': [0.75, 1, 1.25],\n",
    "        'gamma': [5.75, 6, 6.25],\n",
    "        'subsample': [0.9, 0.95, 1.0],\n",
    "        'colsample_bytree': [0.45, 0.5, 0.6],\n",
    "        'max_depth': [2, 3, 4,5,6,7,8]\n",
    "        }\n",
    "folds = 5\n",
    "param_comb = 5\n",
    "\n",
    "# {'subsample': 1.0, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 6, 'colsample_bytree': 0.6}\n",
    "# {'subsample': 1.0, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 6, 'colsample_bytree': 0.5}\n",
    "# {'subsample': 0.95, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 6.25, 'colsample_bytree': 0.6}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc_ovo', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001)\n",
    "\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(random_search.cv_results_)\n",
    "print(random_search.best_estimator_)\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print(random_search.best_params_)\n",
    "\n",
    "y_pred = random_search.best_estimator_.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "macF1 = f1_score(y_test, predictions, average='macro')\n",
    "print(\"Macro-Average_F1:\", macF1)\n",
    "\n",
    "classes = {\n",
    "    0:\"deceased\",\n",
    "    1:\"hospitalized\",\n",
    "    2:\"non-hospitalized\"\n",
    "}\n",
    "\n",
    "#targetClass = 2\n",
    "\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    macDecF1 = f1_score(y_test, predictions, labels=[i], average='macro')\n",
    "    print(\"Macro-Average\", classes[i], \"F1:\", macDecF1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting detection was then done and below is the optimal classifier\\\n",
    "The code for overfitting detection is in 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators=200, objective='multi:softmax', max_depth=19, min_child_weight=0.2)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "xgb_train_accuracy_scores = []\n",
    "xgb_train_f1_scores = []\n",
    "xgb_train_deceased_f1 = []\n",
    "\n",
    "for tIndex, vIndex in folds.split(xgb_X_train, xgb_Y_train):\n",
    "    X_train2, X_val = xgb_X_train.iloc[tIndex], xgb_X_train.iloc[vIndex]\n",
    "    y_train2, y_val = xgb_Y_train.iloc[tIndex], xgb_Y_train.iloc[vIndex]\n",
    "\n",
    "    xgb.fit(xgb_X_train, xgb_Y_train)\n",
    "\n",
    "        \n",
    "    # Make predictions on the validation set\n",
    "    y_pred = xgb.predict(X_val)\n",
    "    \n",
    "    # Calculate the scores\n",
    "    train_accuracy = accuracy_score(y_val, y_pred)\n",
    "    train_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    train_deceased = f1_score(y_val, y_pred, labels=[0], average='macro')\n",
    "\n",
    "\n",
    "    # Append scores to correct list\n",
    "    xgb_train_accuracy_scores.append(train_accuracy)\n",
    "    xgb_train_f1_scores.append(train_f1)\n",
    "    xgb_train_deceased_f1.append(train_deceased)\n",
    "\n",
    "print(\"Average Training accuracy:\", np.mean(xgb_train_accuracy_scores))\n",
    "print(\"Average Training F1:\", np.mean(xgb_train_f1_scores))\n",
    "print(\"Average Training Deceased F1:\", np.mean(xgb_train_deceased_f1))\n",
    "\n",
    "\n",
    "# Predict the validation data and calculate scores\n",
    "xgb_y_pred = xgb.predict(xgb_X_verify)\n",
    "xgb_test_accuracy = accuracy_score(xgb_Y_verify, xgb_y_pred)\n",
    "xgb_test_f1 = f1_score(xgb_Y_verify, xgb_y_pred, average='macro')\n",
    "xgb_test_deceased = f1_score(xgb_Y_verify, xgb_y_pred, labels=[0], average='macro')\n",
    "\n",
    "print(\"Test Accuracy:\", xgb_test_accuracy)\n",
    "print(\"Test Macro F1:\", xgb_test_f1)\n",
    "print(\"Test Deceased F1:\", xgb_test_deceased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date_confirmation</th>\n",
       "      <th>chronic_disease_binary</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>Case_Fatality_Ratio</th>\n",
       "      <th>average_rate</th>\n",
       "      <th>average_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>19.420820</td>\n",
       "      <td>76.050130</td>\n",
       "      <td>0.590604</td>\n",
       "      <td>0</td>\n",
       "      <td>2284.297169</td>\n",
       "      <td>1.942744</td>\n",
       "      <td>1296.647984</td>\n",
       "      <td>1.233688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>14.584244</td>\n",
       "      <td>121.176289</td>\n",
       "      <td>0.724832</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>14.470810</td>\n",
       "      <td>121.427050</td>\n",
       "      <td>0.859060</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>14.580000</td>\n",
       "      <td>121.030000</td>\n",
       "      <td>0.583893</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>13.896130</td>\n",
       "      <td>121.046370</td>\n",
       "      <td>0.590604</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34179</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>14.562220</td>\n",
       "      <td>121.030000</td>\n",
       "      <td>0.697987</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34180</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>7.070000</td>\n",
       "      <td>125.600000</td>\n",
       "      <td>0.757163</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34181</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>14.595800</td>\n",
       "      <td>120.977200</td>\n",
       "      <td>0.588851</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34182</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>14.542117</td>\n",
       "      <td>121.035255</td>\n",
       "      <td>0.662666</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34183</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>10.335943</td>\n",
       "      <td>123.841346</td>\n",
       "      <td>0.570470</td>\n",
       "      <td>0</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "      <td>681.949809</td>\n",
       "      <td>1.779368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34184 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  sex   latitude   longitude  date_confirmation  \\\n",
       "0       65    0  19.420820   76.050130           0.590604   \n",
       "1       73    1  14.584244  121.176289           0.724832   \n",
       "2       25    0  14.470810  121.427050           0.859060   \n",
       "3       54    1  14.580000  121.030000           0.583893   \n",
       "4       60    1  13.896130  121.046370           0.590604   \n",
       "...    ...  ...        ...         ...                ...   \n",
       "34179   58    1  14.562220  121.030000           0.697987   \n",
       "34180   39    1   7.070000  125.600000           0.757163   \n",
       "34181   56    1  14.595800  120.977200           0.588851   \n",
       "34182   54    1  14.542117  121.035255           0.662666   \n",
       "34183   55    1  10.335943  123.841346           0.570470   \n",
       "\n",
       "       chronic_disease_binary  Incident_Rate  Case_Fatality_Ratio  \\\n",
       "0                           0    2284.297169             1.942744   \n",
       "1                           0     681.949809             1.779368   \n",
       "2                           0     681.949809             1.779368   \n",
       "3                           0     681.949809             1.779368   \n",
       "4                           0     681.949809             1.779368   \n",
       "...                       ...            ...                  ...   \n",
       "34179                       0     681.949809             1.779368   \n",
       "34180                       0     681.949809             1.779368   \n",
       "34181                       0     681.949809             1.779368   \n",
       "34182                       0     681.949809             1.779368   \n",
       "34183                       0     681.949809             1.779368   \n",
       "\n",
       "       average_rate  average_ratio  \n",
       "0       1296.647984       1.233688  \n",
       "1        681.949809       1.779368  \n",
       "2        681.949809       1.779368  \n",
       "3        681.949809       1.779368  \n",
       "4        681.949809       1.779368  \n",
       "...             ...            ...  \n",
       "34179    681.949809       1.779368  \n",
       "34180    681.949809       1.779368  \n",
       "34181    681.949809       1.779368  \n",
       "34182    681.949809       1.779368  \n",
       "34183    681.949809       1.779368  \n",
       "\n",
       "[34184 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the file and retreive the data\n",
    "rf_hybrid_data = pd.read_csv(\"Hybrid_data.csv\")\n",
    "#rf_unbalanced_data = pd.read_csv(\"Training_Data.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "rf_X = rf_hybrid_data.drop(columns=['outcome_group'])\n",
    "rf_class_label = rf_hybrid_data['outcome_group']\n",
    "rf_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation data\n",
    "# The training data will be further split using kfold later \n",
    "rf_X_train, rf_X_test, rf_y_train, rf_y_test = train_test_split(rf_X, rf_class_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paramter grid that will be used in grid search to find the best hyper parameters\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "    'max_depth': [10,15,20,25,30,35,40,45],\n",
    "    'min_samples_split': [10, 20, 30,40, 50,100,170,340],\n",
    "}\n",
    "\n",
    "# Create k-folds to be used for training and verification \n",
    "rf_folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### WARNING GRID SEARCH WILL TAKE CLOSE TO 4 HOURS TO RUN ##########\n",
    "# Create random forest classifier\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Create a grid that will be used to tune the hyper parameters\n",
    "rf_grid = GridSearchCV(rfc, rf_param_grid, cv=rf_folds, return_train_score = True, scoring = 'f1_macro')\n",
    "\n",
    "# Try all sets of parameters to find the best combination\n",
    "rf_grid.fit(rf_X, rf_class_label)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(rf_grid.best_params_)\n",
    "print(\"Best score:\")\n",
    "print(rf_grid.best_score_)\n",
    "\n",
    "####### WARNING GRID SEARCH WILL TAKE CLOSE TO 4 HOURS TO RUN ##########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected best paramters are {'max_depth': 40, 'min_samples_split': 10, 'n_estimators': 300}\\\n",
    "The expected best score is 0.9588014236895601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting detection was then done and below is the optimal classifier\\\n",
    "The code for overfitting detection is in 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training accuracy: 0.9489524904037815\n",
      "Average Training F1: 0.947946434525966\n",
      "Average Training Deceased F1: 0.920506507053417\n",
      "Test Accuracy: 0.9474915898786017\n",
      "Test Macro F1: 0.9464534373046676\n",
      "Test Deceased F1: 0.9171348314606742\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 300, min_samples_split = 10, max_depth = 19, random_state=0)\n",
    "rf_train_accuracy_scores = []\n",
    "rf_train_f1_scores = []\n",
    "rf_train_deceased_f1 = []\n",
    "\n",
    "for train_index, val_index in rf_folds.split(rf_X_train, rf_y_train):\n",
    "    X_train2, X_val = rf_X_train.iloc[train_index], rf_X_train.iloc[val_index]\n",
    "    y_train2, y_val = rf_y_train.iloc[train_index], rf_y_train.iloc[val_index]\n",
    "\n",
    "    # Train your classifier\n",
    "    rf_clf.fit(X_train2, y_train2)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred = rf_clf.predict(X_val)\n",
    "    \n",
    "    # Calculate the scores\n",
    "    train_accuracy = accuracy_score(y_val, y_pred)\n",
    "    train_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    train_deceased = f1_score(y_val, y_pred, labels=[0], average='macro')\n",
    "\n",
    "\n",
    "    # Append scores to correct list\n",
    "    rf_train_accuracy_scores.append(train_accuracy)\n",
    "    rf_train_f1_scores.append(train_f1)\n",
    "    rf_train_deceased_f1.append(train_deceased)\n",
    "\n",
    "print(\"Average Training accuracy:\", np.mean(rf_train_accuracy_scores))\n",
    "print(\"Average Training F1:\", np.mean(rf_train_f1_scores))\n",
    "print(\"Average Training Deceased F1:\", np.mean(rf_train_deceased_f1))\n",
    "\n",
    "\n",
    "# Predict the validation data and calculate scores\n",
    "rf_y_pred = rf_clf.predict(rf_X_test)\n",
    "rf_test_accuracy = accuracy_score(rf_y_test, rf_y_pred)\n",
    "rf_test_f1 = f1_score(rf_y_test, rf_y_pred, average='macro')\n",
    "rf_test_deceased = f1_score(rf_y_test, rf_y_pred, labels=[0], average='macro')\n",
    "\n",
    "print(\"Test Accuracy:\", rf_test_accuracy)\n",
    "print(\"Test Macro F1:\", rf_test_f1)\n",
    "print(\"Test Deceased F1:\", rf_test_deceased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 - Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Overfitting detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect overfitting for max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store scores for different values of hyperparams\n",
    "rf_total_train_accuracy =[]\n",
    "rf_total_train_f1 = []\n",
    "rf_total_train_deceased = []\n",
    "\n",
    "rf_total_test_accuracy =[]\n",
    "rf_total_test_f1 = []\n",
    "rf_total_test_deceased = []\n",
    "\n",
    "for i in range(10,31):\n",
    "\n",
    "    # Create and train the model\n",
    "    clf = RandomForestClassifier(n_estimators = 300, min_samples_split = 10, max_depth = i, random_state=0)\n",
    "\n",
    "    rf_train_accuracy_scores = []\n",
    "    rf_train_f1_scores = []\n",
    "    rf_train_deceased_f1 = []\n",
    "\n",
    "    for train_index, val_index in rf_folds.split(rf_X_train, rf_y_train):\n",
    "        X_train2, X_val = rf_X_train.iloc[train_index], rf_X_train.iloc[val_index]\n",
    "        y_train2, y_val = rf_y_train.iloc[train_index], rf_y_train.iloc[val_index]\n",
    "\n",
    "        # Train your classifier\n",
    "        clf.fit(X_train2, y_train2)\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        y_pred = clf.predict(X_val)\n",
    "        \n",
    "        # Calculate the scores\n",
    "        train_accuracy = accuracy_score(y_val, y_pred)\n",
    "        train_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "        train_deceased = f1_score(y_val, y_pred, labels=[0], average='macro')\n",
    "\n",
    "        # Append scores to correct list\n",
    "        rf_train_accuracy_scores.append(train_accuracy)\n",
    "        rf_train_f1_scores.append(train_f1)\n",
    "        rf_train_deceased_f1.append(train_deceased)\n",
    "\n",
    "    # Calculate the mean across the kfolds and append to total list\n",
    "    rf_total_train_accuracy.append(np.mean(rf_train_accuracy_scores))\n",
    "    rf_total_train_f1.append(np.mean(rf_train_f1_scores))\n",
    "    rf_total_train_deceased.append(np.mean(rf_train_deceased_f1))\n",
    "\n",
    "    # Predict the test data and calculate the correct scores\n",
    "    rf_y_pred = clf.predict(rf_X_test)\n",
    "    rf_test_accuracy = accuracy_score(rf_y_test, rf_y_pred)\n",
    "    rf_test_f1 = f1_score(rf_y_test, rf_y_pred, average='macro')\n",
    "    rf_test_deceased = f1_score(rf_y_test, rf_y_pred, labels=[0], average='macro')\n",
    "\n",
    "    # Append the scores to their respective list\n",
    "    rf_total_test_accuracy.append(rf_test_accuracy)\n",
    "    rf_total_test_f1.append(rf_test_f1)\n",
    "    rf_total_test_deceased.append(rf_test_deceased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots with respect to change in max_depth to find signs of overfitting\n",
    "hyperParam_values = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
    "dataLists = {\n",
    "    \"accuracyScores\":{\n",
    "        \"title\":\"Training accuracy vs Testing accuracy\",\n",
    "        \"training\":rf_total_train_accuracy,\n",
    "        \"testing\":rf_total_test_accuracy\n",
    "    },\n",
    "    \"macroF1\":{\n",
    "        \"title\":\"Training macroF1 vs Testing macroF1\",\n",
    "        \"training\":rf_total_train_f1,\n",
    "        \"testing\":rf_total_test_f1\n",
    "    },\n",
    "    \"deceased_F1\":{\n",
    "        \"title\":\"Training deceased F1 vs Testing deceased F1\",\n",
    "        \"training\":rf_total_train_deceased,\n",
    "        \"testing\":rf_total_test_deceased\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot graphs for accuracy, macro f1, and deceased f1\n",
    "for i in dataLists.keys():\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(hyperParam_values, dataLists[i]['training'], label='Training')\n",
    "    ax.plot(hyperParam_values, dataLists[i]['testing'], label='Testing')\n",
    "\n",
    "    ax.set_title(dataLists[i]['title'])\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect overfitting for min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store scores for different values of hyperparams\n",
    "rf_total_train_accuracy =[]\n",
    "rf_total_train_f1 = []\n",
    "rf_total_train_deceased = []\n",
    "\n",
    "rf_total_test_accuracy =[]\n",
    "rf_total_test_f1 = []\n",
    "rf_total_test_deceased = []\n",
    "\n",
    "for i in range(10,201,10):\n",
    "\n",
    "    # Create and train the model\n",
    "    clf = RandomForestClassifier(n_estimators = 300, min_samples_split = i, max_depth = 19, random_state=0)\n",
    "\n",
    "    rf_train_accuracy_scores = []\n",
    "    rf_train_f1_scores = []\n",
    "    rf_train_deceased_f1 = []\n",
    "\n",
    "    for train_index, val_index in rf_folds.split(rf_X_train, rf_y_train):\n",
    "        X_train2, X_val = rf_X_train.iloc[train_index], rf_X_train.iloc[val_index]\n",
    "        y_train2, y_val = rf_y_train.iloc[train_index], rf_y_train.iloc[val_index]\n",
    "\n",
    "        # Train your classifier\n",
    "        clf.fit(X_train2, y_train2)\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        y_pred = clf.predict(X_val)\n",
    "        \n",
    "        # Calculate the scores\n",
    "        train_accuracy = accuracy_score(y_val, y_pred)\n",
    "        train_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "        train_deceased = f1_score(y_val, y_pred, labels=[0], average='macro')\n",
    "\n",
    "        # Append scores to correct list\n",
    "        rf_train_accuracy_scores.append(train_accuracy)\n",
    "        rf_train_f1_scores.append(train_f1)\n",
    "        rf_train_deceased_f1.append(train_deceased)\n",
    "\n",
    "    # Calculate the mean across the kfolds and append to total list\n",
    "    rf_total_train_accuracy.append(np.mean(rf_train_accuracy_scores))\n",
    "    rf_total_train_f1.append(np.mean(rf_train_f1_scores))\n",
    "    rf_total_train_deceased.append(np.mean(rf_train_deceased_f1))\n",
    "\n",
    "    # Predict the test data and calculate the correct scores\n",
    "    rf_y_pred = clf.predict(rf_X_test)\n",
    "    rf_test_accuracy = accuracy_score(rf_y_test, rf_y_pred)\n",
    "    rf_test_f1 = f1_score(rf_y_test, rf_y_pred, average='macro')\n",
    "    rf_test_deceased = f1_score(rf_y_test, rf_y_pred, labels=[0], average='macro')\n",
    "\n",
    "    # Append the scores to their respective list\n",
    "    rf_total_test_accuracy.append(rf_test_accuracy)\n",
    "    rf_total_test_f1.append(rf_test_f1)\n",
    "    rf_total_test_deceased.append(rf_test_deceased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots with respect to change in max_depth to find signs of overfitting\n",
    "hyperParam_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n",
    "dataLists = {\n",
    "    \"accuracyScores\":{\n",
    "        \"title\":\"Training accuracy vs Testing accuracy\",\n",
    "        \"training\":rf_total_train_accuracy,\n",
    "        \"testing\":rf_total_test_accuracy\n",
    "    },\n",
    "    \"macroF1\":{\n",
    "        \"title\":\"Training macroF1 vs Testing macroF1\",\n",
    "        \"training\":rf_total_train_f1,\n",
    "        \"testing\":rf_total_test_f1\n",
    "    },\n",
    "    \"deceased_F1\":{\n",
    "        \"title\":\"Training deceased F1 vs Testing deceased F1\",\n",
    "        \"training\":rf_total_train_deceased,\n",
    "        \"testing\":rf_total_test_deceased\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot graphs for accuracy, macro f1, and deceased f1\n",
    "for i in dataLists.keys():\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(hyperParam_values, dataLists[i]['training'], label='Training')\n",
    "    ax.plot(hyperParam_values, dataLists[i]['testing'], label='Testing')\n",
    "\n",
    "    ax.set_title(dataLists[i]['title'])\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect overfitting for n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store scores for different values of hyperparams\n",
    "rf_total_train_accuracy =[]\n",
    "rf_total_train_f1 = []\n",
    "rf_total_train_deceased = []\n",
    "\n",
    "rf_total_test_accuracy =[]\n",
    "rf_total_test_f1 = []\n",
    "rf_total_test_deceased = []\n",
    "\n",
    "for i in range(50,351,50):\n",
    "\n",
    "    # Create and train the model\n",
    "    clf = RandomForestClassifier(n_estimators = i, min_samples_split = 10, max_depth = 19, random_state=0)\n",
    "\n",
    "    rf_train_accuracy_scores = []\n",
    "    rf_train_f1_scores = []\n",
    "    rf_train_deceased_f1 = []\n",
    "\n",
    "    for train_index, val_index in rf_folds.split(rf_X_train, rf_y_train):\n",
    "        X_train2, X_val = rf_X_train.iloc[train_index], rf_X_train.iloc[val_index]\n",
    "        y_train2, y_val = rf_y_train.iloc[train_index], rf_y_train.iloc[val_index]\n",
    "\n",
    "        # Train your classifier\n",
    "        clf.fit(X_train2, y_train2)\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        y_pred = clf.predict(X_val)\n",
    "        \n",
    "        # Calculate the scores\n",
    "        train_accuracy = accuracy_score(y_val, y_pred)\n",
    "        train_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "        train_deceased = f1_score(y_val, y_pred, labels=[0], average='macro')\n",
    "\n",
    "        # Append scores to correct list\n",
    "        rf_train_accuracy_scores.append(train_accuracy)\n",
    "        rf_train_f1_scores.append(train_f1)\n",
    "        rf_train_deceased_f1.append(train_deceased)\n",
    "\n",
    "    # Calculate the mean across the kfolds and append to total list\n",
    "    rf_total_train_accuracy.append(np.mean(rf_train_accuracy_scores))\n",
    "    rf_total_train_f1.append(np.mean(rf_train_f1_scores))\n",
    "    rf_total_train_deceased.append(np.mean(rf_train_deceased_f1)) \n",
    "    # Predict the test data and calculate the correct scores\n",
    "    rf_y_pred = clf.predict(rf_X_test)\n",
    "    rf_test_accuracy = accuracy_score(rf_y_test, rf_y_pred)\n",
    "    rf_test_f1 = f1_score(rf_y_test, rf_y_pred, average='macro')\n",
    "    rf_test_deceased = f1_score(rf_y_test, rf_y_pred, labels=[0], average='macro')\n",
    "\n",
    "    # Append the scores to their respective list\n",
    "    rf_total_test_accuracy.append(rf_test_accuracy)\n",
    "    rf_total_test_f1.append(rf_test_f1)\n",
    "    rf_total_test_deceased.append(rf_test_deceased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots with respect to change in max_depth to find signs of overfitting\n",
    "hyperParam_values = [50, 100, 150, 200, 250, 300, 350]\n",
    "dataLists = {\n",
    "    \"accuracyScores\":{\n",
    "        \"title\":\"Training accuracy vs Testing accuracy\",\n",
    "        \"training\":rf_total_train_accuracy,\n",
    "        \"testing\":rf_total_test_accuracy\n",
    "    },\n",
    "    \"macroF1\":{\n",
    "        \"title\":\"Training macroF1 vs Testing macroF1\",\n",
    "        \"training\":rf_total_train_f1,\n",
    "        \"testing\":rf_total_test_f1\n",
    "    },\n",
    "    \"deceased_F1\":{\n",
    "        \"title\":\"Training deceased F1 vs Testing deceased F1\",\n",
    "        \"training\":rf_total_train_deceased,\n",
    "        \"testing\":rf_total_test_deceased\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot graphs for accuracy, macro f1, and deceased f1\n",
    "for i in dataLists.keys():\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(hyperParam_values, dataLists[i]['training'], label='Training')\n",
    "    ax.plot(hyperParam_values, dataLists[i]['testing'], label='Testing')\n",
    "\n",
    "    ax.set_title(dataLists[i]['title'])\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "xgb_final_predictions = xgb.predict(unlabelled_data)\n",
    "\n",
    "def create_submission_file(y_preds, file_name):\n",
    "    with open(file_name, \"w\") as csvfile:\n",
    "        wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow([\"Id\", \"Prediction\"])\n",
    "        for i, pred in enumerate(y_preds):\n",
    "            wr.writerow([str(i), str(pred)])\n",
    "create_submission_file(xgb_final_predictions, \"submission_XGBoost.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
